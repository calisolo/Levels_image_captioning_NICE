# Levels
Segment importance of hints seen by model to natural language token 'Levels'

# Abstract

This project was transformed based on OFA Chinese and challenged the **NICE (New frontiers for zero-shot Image Captioning Evaluation)** challenge 2023, resulting in **Track2 2nd/ Total 4th**. (**CVPR 2023 Workshop**)
NICE is an Image Captioning Task, which is a task to create appropriate captions for each photo provided by ShutterStock. Based on the intuition that the tone of caption in the NICE dataset feels unique, it was approached from the perspective of controlled dialogue generation.

ë³¸ í”„ë¡œì íŠ¸ëŠ” OFA Chineseë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³€í˜•í•˜ì—¬ **NICE(New frontiers for zero-shot Image Captioning Evaluation)** challenge 2023 ë¥¼ ë„ì „í•˜ì—¬ **Track2 2nd/ Total 4th**ì˜ ì„±ê³¼ë¥¼ ë‚´ì—ˆìŠµë‹ˆë‹¤. (**CVPR 2023 Workshop**)
NICEëŠ” Image Captioning Task ë¡œ, ShutterStock ì‚¬ì—ì„œ ì œê³µí•œ ê° ì‚¬ì§„ì— ì•Œë§ëŠ” ìº¡ì…˜ì„ ìƒì„±í•˜ëŠ” ê³¼ì œì…ë‹ˆë‹¤. NICE dataset ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë§íˆ¬ê°€ íŠ¹ì´í•˜ê²Œ ëŠê»´ì§„ë‹¤ëŠ” ì§ê´€ì„ ë°”íƒ•ìœ¼ë¡œ, ì´ë¥¼ controlled dialogue generation ê´€ì ì—ì„œ ì ‘ê·¼í•˜ì˜€ìŠµë‹ˆë‹¤.

Editing :joy_cat::joy_cat::joy_cat:

# Quick Start 

Utilize preprocessed cosine similarities, trained models, etc.<br>
You can check the submission creating procedure, output captions of each photo, input data format looking through model inferencing code below.<br>

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/calisolo/Levels_image_captioning_NICE/blob/master/NICE_quickstart.ipynb)



## Main task
- Since this approach is a methodology that connects the features of image captions with well-trained image encoder features, we utilized the open license model OFA, which has proven high performance.
- I wanted to create and train normalized hint level tokens so that the model could understand them.
- model checkpoint transition from fairseq style to huggingface style checkpoint, I refer to the code below and give credit.
- [Checkpoint transition](https://colab.research.google.com/drive/1LLJewY92LXdeug5m_ceMUHdlqrRQwSQJ?usp=sharing)
 fairseq style -> hf style
 
 
## Reproduce from scratch

### 0. Dataset characteristics & Preprocess
When looking at the groundtruth caption, there were many captions that explained the **format of the photo in the prefix** or described a **specific location**.
To identify trends, manually tagging was performed on 5000 cases as follows. (6-8 hours) ğŸ‘·â€â™‚ï¸ğŸ‘·â€â™‚ï¸
|         caption_gt            | photo style prefix                                       | location at the caption                             |
|------------------------------|-----------------------------------------------------------|-----------------------------------------------------|
| Close up low angle view of Bicycles leaning against tree in wood| Close up low angle view of | NULL |
| View of town and bridge spanning river on sunny day Jarnac and the Charente river West Central France | View of | Jarnac and the Charente river West Central France|
| Sun beach and ocean at Gerrans Bay Cornwall United Kingdom | NULL |   Gerrans Bay Cornwall United Kingdom  |

[original validation set](https://github.com/calisolo/Levels_image_captioning_NICE/blob/master/data/nice-val-5k.csv) <br>
[tagged validation set](https://github.com/calisolo/Levels_image_captioning_NICE/blob/master/data/shotstyle_location%20worked.csv)
 
**Hypothesis**
1. Photos provided by the same supplier can be inferred through the information inherent in the image, and the subject/photo/caption method will be similar.
2. Public id is shutterstock's upload number, and it is highly likely that the photos uploaded consecutively have the same supplier.

=> Learning by using similarity between photos and public id provided in Validation_set
 
I use the NICE validation dataset as training data. The dataset consists of two files: caption data and image data. <br>
The training data consists of NICE validation data(5000 cases) and the test data consists of NICE test data (21377 cases). <br>
Caption data stores hints constructed based on id similarity and image cosine similarity, and **levels** meaning the strength of the hint.

<details>
<summary>(click!)How to make encoder_prefix (Input data format using Levels)</summary>
<br><br>
Based on the degree of similarity in the encoder part of the model, i tried to provide captions of several similar photos and hint levels using special tokens to show how similar the corresponding photos and the querying photo are.
Below are the criteria for judging the hint 'Levels'.

|     hint Levels(special tokens)  | Degree of hint effect                               | criterion                     |
|------------------------------|-----------------------------------------------------------|-----------------------------------------------------|
| [cosHint lv4] | Strong hints for nearly identical photos | cosine similarities >0.4 |
| [cosHint lv3] | Same topic but expected to have different captions | cosine similarities >0.32 |
| [cosHint lv2] | Similar photos but different captions | cosine similarities >0.29 |
| [cosHint lv1] | Irrelevant photos | cosine similarities â‰¤ 0.29 |
| [diffHint lv3] | The public_id difference between the photos is very small | id difference < 100 |
| [diffHint lv2] | The public_id difference between the photos is small  | id difference < 10000 |
| [diffHint lv1] | The public_id difference between the photos is large  | id difference â‰¥ 10000 |
 
The above hints were extracted from similar photos obtained based on cosine similarity, and the tagged shotstyles and locations were extracted from neighboring photos obtained based on id_difference.
<br><br>
</details>

caption data ï¼Œjsonl formatï¼š
```
{"image_id": "1813180760", "text": ["A vertical shot of sunset on a beach"], "encoder_prefix": "[cosHint lv3][diffHint lv1]A landscape shot of sunset at horizon over ocean[cosHint lv3][diffHint lv1]Sun beach and ocean at Gerrans Bay Cornwall United Kingdom[cosHint lv3][diffHint lv1]Vertical shot of a beautiful sunset over the sea[cosHint lv3][diffHint lv1]Sunrise near Los Islotes Baja California Sur Mexico"}
{"image_id": "1578946151", "text": ["A woman relaxing in a deck chair"], "encoder_prefix": "[cosHint lv3][diffHint lv2]A woman relaxing in a deck chair[cosHint lv3][diffHint lv1]Wide shot of a female in swimwear walking on the beach with an equipment bucket[cosHint lv3][diffHint lv1]A man meditating by a pool[cosHint lv2][diffHint lv1]Vertical shot of a woman in swimwear standing in water at the shore of a sunny beach"}
```

image dataï¼Œtsv format (img_id, '\t', img_content)ï¼ˆbase64 formatï¼‰ï¼š
```
1813180760 /9j/4AAQSkZJRgABAQAAAQABAAD/2w...
1578946151 /9j/4AAQSkZJRgABAQAAAQABAAD/2w...
```
**preprocess** <br>
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/calisolo/Levels_image_captioning_NICE/blob/master/OFA_prepare_data.ipynb)
<br>

### 1. Make Tokenizer and Train at Colab

Create a tokenizer that adds special tokens representing the strength of the hint as levels.<br>
After adjusting 'train_args', put the picture and hint level into the encoder. Feed the image caption output into the decoder and start training to predict captions.

### environment
transformers==4.20.0

### training script
```
CUDA_VISIBLE_DEVICES=0 python train.py --train_args_file train_args/train_ofa.json
```


### Model Checkpoints
|         Model             | introduction                                              | Link                                               |
|------------------------------|-----------------------------------------------------------|-----------------------------------------------------|
| calisolo/OFA_huge_image_captioning| Optimized checkpoints for image captioning in the OFA-SYS | https://huggingface.co/calisolo/OFA_huge_image_captioning |
| calisolo/OFA_huge_NICE_captioning | One fine-tuned checkpoint with good progress when heuristically looked at | https://huggingface.co/calisolo/OFA_huge_NICE_captioning |
| submission 3 checkpoint       | need to be reproduced with following train_args          |       |
| submission 4 checkpoint    | need to be reproduced with following train_args        |     |
| Ensemble 1 checkpoint   | need to be reproduced with following train_args         |  |


## Code Details

### Repository structure
- data: Data (Cosine Similarities/ input data/ ground truth validation sets)
- imagesï¼š input images (base64 format)
- component:
  - ofa:ofa model architecture
  - argument.pyï¼štrain parameter
  - datacollator.py
  - dataset.py
- train_argsï¼štrain parameter configuration
- vocabï¼štokenizer with 'Levels' token added
<br>

- convert_weight.pyï¼šCheckpoint transition/ but didn't found, didn't used   ğŸ˜¿ğŸ˜¿ 
- generate.py: model generate example/ didn't used








## Cherry picked examples

| example                                          | submission 3 |  submission 4    |  submission 5 |
|---------------------------------------------|:-------------------------:|:---:|:------------------:|
| <img src="./images/test/earrings.jpg" width="160"> |        ç²¾è‡´å°è€³é’‰ï¼Œç‚¹ç¼€ä½ çš„ç¾        |  è€³ç’°,å¤¾å¼è€³ç’°espritoutletå°åŒ—è€³é£¾,è€³ç’°   |   å°è€³é’‰ï¼Œè®©ä½ çš„è€³æœµæ›´æœ‰æ°”è´¨    |
| <img src="./images/test/necklace.jpg" width="160" > |      ç²¾è‡´é”éª¨é“¾ï¼Œç‚¹ç¼€é¢ˆé—´çš„å°æ€§æ„Ÿ       |  é¡¹é“¾é¡¹é“¾è®¾è®¡çŸ¢é‡çŸ¢é‡å›¾ç´ æç¬¬1é¡µ   |   ç²¾è‡´é”éª¨é“¾ï¼Œå½°æ˜¾å¥³æ€§ä¼˜é›…æ°”è´¨   |
| <img src="./images/test/berets.jpg" width="160" > |       å¤å¤è´é›·å¸½ï¼Œæ¼”ç»ç§‹å†¬æ–°æ—¶å°š       |  å¸½å­å¥³ç§‹å†¬æ–°æ¬¾éŸ©ç‰ˆæ—¶å°šç™¾æ­ç¾Šæ¯›å‘¢è´é›·   |     é’ˆç»‡å¼€è¡«ï¼Œæ¸©æš–åˆæ—¶é«¦     |
| <img src="./images/test/glasses.jpg" width="160" > |      å¤å¤çœ¼é•œæ¡†ï¼Œæˆ´å‡ºä½ çš„æ½®æµèŒƒå„¿       |  æˆ´çœ¼é•œçš„å¥³ç”Ÿå¤´åƒ_www.qqya.com   |    é»‘è‰²æ¯›å‘¢å¤–å¥—ï¼Œæ—¶é«¦åˆæ˜¾ç˜¦    |
| <img src="./images/test/manicure.jpg" width="160" > |    å°æ¸…æ–°æ‰‹ç»˜ç¾ç”²ï¼Œè®©ä½ çš„æŒ‡å°–å……æ»¡è‰ºæœ¯æ„Ÿ     |  ç¾ç”²å›¾ç‰‡å¤§å…¨å¯çˆ±å›¾ç‰‡_www.qqya.com   |   ç¾ç”²æŒ‡ç”²æ²¹ï¼Œè®©ä½ çš„æŒ‡ç”²æ›´ç¾ä¸½   |
| <img src="./images/test/lipstick.jpg" width="160" > |      é«˜é¢œå€¼å£çº¢ï¼Œè®©ä½ çš„å”‡è‰²æ›´åŠ è¯±äºº      |  é¦™å¥ˆå„¿chanelé¦™å¥ˆå…’é¦™æ°´é¦™æ°›ç³»åˆ—é¦™æ°´ç¦®ç›’é¦™   |    é«˜é¢œå€¼å£çº¢ï¼Œè®©ä½ çˆ±ä¸é‡Šæ‰‹    |
| <img src="./images/test/beauty-egg.jpg" width="160" > |       é«˜é¢œå€¼ç¾å¦†è›‹ï¼Œæ‰“é€ ç²¾è‡´å¦†å®¹       |  æ—¥æœ¬canmakeäº•ç”°èœœç²‰é¥¼æ§æ²¹å®šå¦†æŒä¹…é®ç‘•æ§æ²¹   |  é«˜é¢œå€¼ç¾å¦†è›‹ï¼Œè½»æ¾æ‰“é€ æ°”è´¨å¥³ç¥   |
| <img src="./images/test/concealer-brush.jpg" width="160" > |       åŒ–å¦†åˆ·é€‰çš„å¥½ï¼Œå¦†å®¹æ²¡çƒ¦æ¼        |  æ—¥æœ¬mujiæ— å°è‰¯å“æ¶¦å”‡è†ä¿æ¹¿æ»‹æ¶¦å”‡éƒ¨æŠ¤ç†   |  ç§‹å†¬å­£èŠ‚ï¼Œä½ éœ€è¦ä¸€æ¬¾å¥½çœ‹çš„çœ¼å½±ç›˜  |
| <img src="./images/test/skirt.jpg" width="160" > |       æ—¶å°šç™¾è¤¶è£™ï¼Œè®©ä½ ç¾å‡ºæ–°é«˜åº¦       |  ç™¾è¤¶è£™åŠèº«è£™å¥³ç§‹å†¬2020æ–°æ¬¾éŸ©ç‰ˆé«˜è…°aå­—   | æ—¶å°šç™¾æ­çš„åŠèº«è£™ï¼Œè®©ä½ è½»æ¾ç©¿å‡ºå¥³ç¥èŒƒ |
| <img src="./images/test/high-heel.jpg" width="160" > |       å°–å¤´é«˜è·Ÿé‹ï¼Œç©¿å‡ºä¼˜é›…å¥³äººå‘³       |  shoesirizachristianlouboutin   |  æ—¶å°šå°–å¤´é«˜è·Ÿé‹ï¼Œç©¿å‡ºä¼˜é›…å¥³äººå‘³   |
| <img src="./images/test/socks.jpg" width="160" > |    åŠ åšçº¯æ£‰è¢œå­å¥³ï¼Œå†¬å­£ä¸­ç­’è¢œå­¦ç”Ÿå †å †è¢œ     |  åŠ åšç¾Šç»’è¢œå­å¥³ä¸­ç­’è¢œå†¬å­£åŠ ç»’ä¿æš–æ£‰è¢œ   |    åŠ åšç¾Šç»’è¢œï¼Œä¿æš–åˆèˆ’é€‚     |
| <img src="./images/test/red-dress.jpg" width="160" > |        åŠå¸¦è¿è¡£è£™ï¼Œæ¸…å‡‰ä¸€å¤         |  æ—¥ç³»å°æ¸…æ–°ç”œç¾å¯çˆ±å°‘å¥³ç³»å­¦é™¢é£å°çº¢è£™   |   ä¸€å­—è‚©è¿è¡£è£™ï¼Œç©¿å‡ºå¥³ç¥èŒƒå„¿    |
| <img src="./images/test/bra.jpg" width="160" > |       å†…è¡£å¥—è£…ï¼Œç»™ä½ è´´å¿ƒçš„å‘µæŠ¤        |  çº¢è‰²èƒŒæ™¯ä¸Šçš„å¥³æ€§æ‰‹æ‹¿ç€ä¸€ä¸ªçº¢è‰²çš„å¤§è±¡   | çº¢è‰²å©šåº†ç”¨å“ï¼Œè®©ä½ çš„å©šç¤¼æ›´æœ‰ä»ªå¼æ„Ÿ  |
| <img src="./images/test/toy-dog.jpg" width="160" > |      å„¿ç«¥æ¯›ç»’ç©å…·ï¼Œé™ªä¼´å®å®å¿«ä¹æˆé•¿      |  ã€éœ‡æ’¼ç²¾å“ç™¾è²¨ã€‘mickymouse_ç±³å¥‡ç±³å¦®~   |  å¯çˆ±å¡é€šæ¯›ç»’ç©å…·ï¼ŒèŒåŒ–ä½ çš„å°‘å¥³å¿ƒ  |
| <img src="./images/test/apple.jpg" width="160" > |     çƒŸå°çº¢å¯Œå£«è‹¹æœï¼Œè„†ç”œå¤šæ±ï¼Œé¦™ç”œå¯å£     |  å±±ä¸œçƒŸå°æ –éœçº¢å¯Œå£«è‹¹æœæ–°é²œæ°´æœå½“å­£æ•´   |    æ–°é²œæ°´æœï¼Œè®©ä½ çˆ±ä¸é‡Šæ‰‹     |
| <img src="./images/test/cake.jpg" width="160" > |      è‰è“å¥¶æ²¹è›‹ç³•ï¼Œæ»¡è¶³ä½ çš„å°‘å¥³å¿ƒ       |  è‰è“å¥¶æ²¹è›‹ç³•å›¾ç‰‡   |   ç¾å‘³çš„ç”Ÿæ—¥è›‹ç³•ï¼Œè®©ä½ çˆ±ä¸é‡Šæ‰‹   |
| <img src="./images/test/bread.jpg" width="160" > |        æ‰‹æ’•é¢åŒ…ï¼Œè¥å…»åˆç¾å‘³         |  é¢åŒ…åŒ…è£…ç›’è®¾è®¡   | å¥½åƒåˆ°åœä¸ä¸‹æ¥çš„æ‰‹æ’•é¢åŒ…ï¼Œä½ åƒè¿‡å—ï¼Ÿ |
| <img src="./images/test/biscuit.jpg" width="160" > |     é¦™è„†è–„è„†é¥¼å¹²ï¼Œè®©ä½ åœä¸ä¸‹æ¥çš„ç¾å‘³      |  éŸ©é¦™æµ·è‹”å‘³è–„è„†åŠ è–¯ç‰‡ä¼‘é—²é›¶é£Ÿå°åƒè†¨åŒ–   |    ç¾å‘³é›¶é£Ÿï¼Œè®©ä½ çˆ±ä¸é‡Šæ‰‹     |
| <img src="./images/test/sweeping-robot.jpg" width="160" > |      æ™ºèƒ½æ‰«åœ°æœºå™¨äººï¼Œè®©å®¶æ›´å¹²å‡€æ•´æ´      |  å°ç±³ç±³å®¶æ‰«åœ°æœºå™¨äººæ™ºèƒ½å®¶ç”¨å…¨è‡ªåŠ¨å¸å°˜   |  æ™ºèƒ½æ‰«åœ°æœºå™¨äººï¼Œè®©ç”Ÿæ´»æ›´æœ‰ä»ªå¼æ„Ÿ  |
| <img src="./images/test/iphone11.jpg" width="160" > |     è‹¹æœ11promaxï¼Œæ€§ä»·æ¯”è¶…é«˜      |  è‹¹æœ11æ‰‹æœºå£³iphone11promaxä¿æŠ¤å¥—ç¡…èƒ¶å…¨åŒ…è¾¹   |    é«˜é¢œå€¼æ‰‹æœºï¼Œä½ å€¼å¾—æ‹¥æœ‰     |
| <img src="./images/test/washing-machine.jpg" width="160" > |       æ™ºèƒ½æ´—è¡£æœºï¼Œæ´—å‡ºå¥åº·å¥½ç”Ÿæ´»       |  æ´—è¡£æœºå›¾æ ‡éš”ç¦»åœ¨ç™½è‰²èƒŒæ™¯ä¸Šã€‚3dæ¸²æŸ“ã€‚   |  æ™ºèƒ½æ´—è¡£æœºï¼Œè®©ä½ çš„ç”Ÿæ´»æ›´æœ‰ä»ªå¼æ„Ÿ  |
| <img src="./images/test/power-bank.jpg" width="160" > |    æ—¶å°šå……ç”µå®ï¼Œè®©ä½ çš„æ‰‹æœºå……ç”µæ›´å¿«æ›´å®‰å…¨     |  å°ç±³ç§»åŠ¨ç”µæº10000æ¯«å®‰è¶…å¤§å®¹é‡å……ç”µå®   |  é«˜é¢œå€¼å……ç”µå®ï¼Œè®©ä½ çš„æ‰‹æœºå……ç”µæ›´å¿«  |
| <img src="./images/test/shoes.jpg" width="160" > |       æ—¶å°šè¿åŠ¨é‹ï¼Œè®©ä½ è¿åŠ¨æ›´è‡ªä¿¡       |  ç‰¹æ­¥ä¸“æŸœæ¬¾ç”·å­å¤å­£è·‘é‹17æ–°å“æ°”å«å‡éœ‡   |  èˆ’é€‚è·‘æ­¥é‹ï¼Œè®©ä½ è½»æ¾è·‘å‡ºå¥½èº«æ   |
| <img src="./images/test/denim-jacket.jpg" width="160" > |      æ—¶å°šæ½®æµèµ„è®¯ï¼Œå‹ç”·æŠŠå¦¹çº¦ä¼šå¤¹å…‹      |  ç”·ç«¥å¤–å¥—æ˜¥ç§‹å­£æ–°æ¬¾éŸ©ç‰ˆå„¿ç«¥å¤¹å…‹ä¸­å¤§ç«¥   |   æ—¶å°šæ½®æµï¼Œå‹ç”·åŸåˆ›ä¼‘é—²è¡¬è¡«    |
| <img src="./images/test/hoodie.jpg" width="160" > |      æ—¶å°šçµæ„ŸæŒ‡å—ï¼Œå‹ç”·åŸåˆ›è¡—æ‹å«è¡£      |  ç”·å£«é•¿è¢–tæ¤ç§‹å­£æ–°æ¬¾éŸ©ç‰ˆæ½®æµå®½æ¾åœ†é¢†   |  æ—¶å°šçµæ„ŸæŒ‡å—ï¼Œå‹ç”·åŸåˆ›æ½®æµå«è¡£   |


## Reference

Backbone model 
- [OFAï¼šUnifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework](https://arxiv.org/pdf/2202.03052.pdf)  
- [OFA github](https://github.com/OFA-Sys/OFA)


![ofa-task](./images/ofa-task.png)

codebase
- [OFA-Chinese github](https://github.com/yangjianxin1/OFA-Chinese) 
- [OFA-Chinese detail](https://mp.weixin.qq.com/s/thRbR1i6cZk8zUz3y2mq6g)

### Description of the OFA Chinese
- The OFA-sys official codebase has a high degree of complexity to be compatible with several experimental configurations. OFA Chinese is a huggingface version of the fine-tuning code that leaves only the core logic.


